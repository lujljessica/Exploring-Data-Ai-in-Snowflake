{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark.functions import row_number, col\n",
    "from snowflake.snowpark.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of splitting a Snowflake Dataframe without shuffling \n",
    "Good for ordered data like time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(session: snowpark.Session): \n",
    "    tableName = 'silver.daily_revenue'\n",
    "    df = session.table(tableName)\n",
    "    window_spec = Window.order_by(col(\"TRANSACTION_DATE\"))\n",
    "    df_with_index = df.with_column(\"row_num\", row_number().over(window_spec))\n",
    "    total_rows = df_with_index.count()\n",
    "    train_size = int(total_rows * 0.7)\n",
    "    train_df = df_with_index.filter(col(\"row_num\") <= train_size).drop(\"row_num\")\n",
    "    test_df = df_with_index.filter(col(\"row_num\") > train_size).drop(\"row_num\")\n",
    "    \n",
    "    return train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SPROC for model training\n",
    "Creates a Stored to procedure that trains a model and saves it to the model registry\n",
    "\n",
    "To be used in Snowflake Python Worksheet\n",
    "\n",
    "All variables that must be changed is tagged with #edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "import snowflake.snowpark\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.functions import sproc\n",
    "import snowflake.snowpark.types as T\n",
    "\n",
    "# Snowpark ML\n",
    "from snowflake.ml.registry import registry\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "import snowflake.ml.modeling.preprocessing as snowmlpp\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "\n",
    "sproc_name = ''\n",
    "stage_name = ''\n",
    "@sproc(name=sproc_name, #edit\n",
    "       stage_location=stage_name,  #edit\n",
    "       is_permanent=True, \n",
    "       replace=True, \n",
    "       packages=[\n",
    "        \"snowflake-snowpark-python\",\n",
    "        'snowflake-ml-python', \n",
    "        'xgboost',\n",
    "        'pandas', \n",
    "         ])\n",
    "def train_and_save_model(session: Session, source_table: str, major_version: bool = True) -> str:\n",
    "    # setting variables \n",
    "    model_name = 'silver.daily_revenue_test'\n",
    "    train_vw_name =  'silver.vw_daily_revenue_train'\n",
    "    target_cols = ['TOTAL_AMOUNT'] \n",
    "    output_cols = ['PREDICTED_AMOUNT']    \n",
    "    # read in training and test data \n",
    "    train_df = session.table(train_vw_name)\n",
    "    test_df = session.table(test_df)\n",
    "    \n",
    "    #Join in date features\n",
    "    ts_features = session.table('Retail_demo.Silver.Date_features')\n",
    "    train_df = train_df.join(ts_features, ['TRANSACTION_DATE'])\n",
    "    train_df = train_df.drop(\"Transaction_date\")\n",
    "\n",
    "    test_df = test_df.join(ts_features, ['TRANSACTION_DATE'])\n",
    "    test_df = test_df.drop(\"Transaction_date\")\n",
    "\n",
    "    other_cols = [i for i in train_df.schema.names if i not in target_cols]\n",
    "    numeric_types = [T.DecimalType, T.DoubleType, T.FloatType, T.IntegerType, T.LongType]\n",
    "    features = [col.name for col in train_df.schema.fields if (type(col.datatype) in numeric_types) and (col.name in other_cols)]\n",
    " \n",
    "    model_pipe = Pipeline(\n",
    "                            steps=[\n",
    "                                ('grid_search_reg', GridSearchCV(estimator=XGBRegressor(),\n",
    "                                                                    param_grid={ \"n_estimators\":[50, 100, 200], # 25\n",
    "                                                                                \"learning_rate\":[0.01, 0.1, 0.5 ], # .5\n",
    "                                                                                },\n",
    "                                                                    n_jobs = -1,\n",
    "                                                                    scoring=\"neg_mean_squared_error\",\n",
    "                                                                    input_cols=features,\n",
    "                                                                    label_cols=target_cols,\n",
    "                                                                    output_cols=output_cols\n",
    "                                                                    )\n",
    "                                )\n",
    "                            ]      \n",
    "                        )\n",
    "    model_pipe.fit(train_df)\n",
    "    results = model_pipe.predict(test_df)\n",
    "\n",
    "    mape = mean_absolute_percentage_error(df=results, y_true_col_names=target_cols, y_pred_col_names=output_cols)\n",
    "    mse = mean_squared_error(df=results, y_true_col_names=target_cols, y_pred_col_names=output_cols)\n",
    "    def set_model_version(registry_object,model_name, major_version=True):\n",
    "        import numpy as np\n",
    "        import json\n",
    "            \n",
    "        model_list = registry_object.show_models()\n",
    "        model_list_filter = model_list[model_list['name'] ==  model_name]\n",
    "        if (len(model_list) == 0) or (len(model_list_filter) == 0):\n",
    "            return 'V1'\n",
    "        version_list_string = model_list_filter['versions'].iloc[0]\n",
    "        version_list = json.loads(version_list_string)\n",
    "        version_numbers = [float(s.replace('V', '')) for s in version_list]\n",
    "        model_last_version = max(version_numbers)\n",
    "                \n",
    "        if np.isnan(model_last_version) == True:\n",
    "            model_new_version = 'V1'\n",
    "        elif np.isnan(model_last_version) == False and major_version == True:\n",
    "            model_new_version = round(model_last_version + 1,2)\n",
    "            model_new_version = 'V' + str(model_new_version)\n",
    "            \n",
    "        else:\n",
    "            model_new_version = round(model_last_version + .1,2)\n",
    "            model_new_version = 'V' + str(model_new_version)\n",
    "                    \n",
    "        return model_new_version \n",
    "    try:\n",
    "        model_registry = registry.Registry(session=session, database_name=session.get_current_database(), schema_name='ML_PIPE')\n",
    "\n",
    "    except Exception as e:\n",
    "        return (f'Error with creating model registry object: {e}')\n",
    "                \n",
    "    try:\n",
    "        target_cols = ['TOTAL_AMOUNT'] #edit\n",
    "        feature_cols = [i for i in test_df.schema.names if i not in target_cols]\n",
    "        X = train_df.select(feature_cols).limit(100)\n",
    "        model_name = model_name\n",
    "        version_name = set_model_version(model_registry, model_name, major_version=major_version)\n",
    "        model_version = model_registry.log_model(\n",
    "            model = model_pipe, \n",
    "            model_name = model_name, \n",
    "            version_name= f'\"{version_name}\"',\n",
    "            sample_input_data=X,\n",
    "            conda_dependencies=['snowflake-snowpark-python','snowflake-ml-python','scikit-learn', 'xgboost']\n",
    "            )\n",
    "\n",
    "        model_version.set_metric(metric_name='mean_abs_pct_err', value=mape)\n",
    "        model_version.set_metric(metric_name='mean_sq_err', value=mse)\n",
    "                \n",
    "    except Exception as e:\n",
    "        return (f'Error with saving model to registry: {e}')\n",
    "    try:\n",
    "        session.sql(f'alter model {model_name} set default_version = \"{version_name}\";')\n",
    "    except Exception as e:\n",
    "        return (f'Error with setting default version: {e}')\n",
    "\n",
    "    return f'Model {model_name} has been logged with version {version_name} and has a MAPE of {mape} and MSE of {mse}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and create a session\n",
    "from snowflake.snowpark import Session\n",
    "import snowflake.snowpark\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.functions import sproc\n",
    "import snowflake.snowpark.types as T\n",
    "from snowflake import telemetry\n",
    "\n",
    "from opentelemetry import trace\n",
    "\n",
    "# Snowpark ML\n",
    "from snowflake.ml.registry import registry\n",
    "\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "import snowflake.ml.modeling.preprocessing as snowmlpp\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "\n",
    "sproc_name = ''\n",
    "stage_name = ''\n",
    "@sproc(name=sproc_name, #edit\n",
    "       stage_location=stage_name,  #edit\n",
    "       is_permanent=True, \n",
    "       replace=True, \n",
    "       packages=[\n",
    "        \"snowflake-snowpark-python\",\n",
    "        'snowflake-ml-python', \n",
    "        'xgboost',\n",
    "        'pandas', \n",
    "        \"snowflake-telemetry-python\",  # Required for tracing\n",
    "        \"opentelemetry-api\" # Required for tracing\n",
    "        ])\n",
    "def train_and_save_model(session: Session, source_table: str, major_version: bool = True) -> str:\n",
    "    model_name = ''\n",
    "    train_df = ''\n",
    "    test_df = ''\n",
    "    \n",
    "    tracer = trace.get_tracer(\"daily_revenue.train\")\n",
    "    with tracer.start_as_current_span(sproc_name) as main_span: #edit\n",
    "        try:\n",
    "            telemetry.set_span_attribute(\"model.name\", model_name)\n",
    "            # Data loading\n",
    "            with tracer.start_as_current_span(\"data_loading\"):\n",
    "                try:\n",
    "                    df = session.table(source_table)#.limit(100000) # only need to limit if the data is huge\n",
    "                    telemetry.set_span_attribute(\"data.row_count\", df.count()) \n",
    "                except Exception as e:\n",
    "                    return (f'Error with getting table data: {e}')\n",
    "\n",
    "            with tracer.start_as_current_span(\"feature_engineering\"):\n",
    "                target_cols = ['TOTAL_AMOUNT'] #edit\n",
    "                feature_cols = [i for i in df.schema.names if i not in target_cols]\n",
    "                output_cols = ['PREDICTED_AMOUNT'] #edit\n",
    "\n",
    "                # Define Snowflake numeric types (possibly for scaling, ordinal encoding)\n",
    "                '''\n",
    "                numeric_types = [T.DecimalType, T.DoubleType, T.FloatType, T.IntegerType, T.LongType]\n",
    "                numeric_columns = [col.name for col in df.schema.fields if (type(col.datatype) in numeric_types) and (col.name in feature_cols)]\n",
    "                '''\n",
    "                # Define Snowflake categorical types and determine which columns to OHE\n",
    "                '''\n",
    "                categorical_types = [T.StringType]\n",
    "                cols_to_ohe = [col.name for col in df.schema.fields if (type(col.datatype) in categorical_types)]\n",
    "                ohe_cols_output = [col + '_OHE' for col in cols_to_ohe]\n",
    "                '''\n",
    "                # Standardize the values in the rows by removing spaces, capitalizing\n",
    "                def fix_values(columnn):\n",
    "                        return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n",
    "                \n",
    "                '''\n",
    "                try:\n",
    "                    for col in cols_to_ohe:\n",
    "                            df = df.na.fill('NONE', subset=col)\n",
    "                            df = df.withColumn(col, fix_values(col))\n",
    "                    telemetry.add_event(\"feature_engineering_complete\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    return (f'Error with standardizing values: {e}')\n",
    "                '''\n",
    "                telemetry.add_event(\"feature_engineering_complete\")\n",
    "\n",
    "            # Model training\n",
    "            with tracer.start_as_current_span(\"define_pipeline\"):\n",
    "                try:\n",
    "                    pipe = Pipeline(\n",
    "                        steps=[\n",
    "                            #('imputer', SimpleImputer(input_cols=all_cols)),\n",
    "                            #('mms', snowmlpp.MinMaxScaler(input_cols=cols_to_scale, output_cols=scale_cols_output)),\n",
    "                            #('ohe', snowmlpp.OneHotEncoder(input_cols=cols_to_ohe, output_cols=ohe_cols_output, drop_input_cols=True)),\n",
    "                            ('grid_search_reg', GridSearchCV(estimator=XGBRegressor(),\n",
    "                                                                param_grid={ \"n_estimators\":[50, 100, 200], # 25\n",
    "                                                                            \"learning_rate\":[0.01, 0.1, 0.5 ], # .5\n",
    "                                                                            },\n",
    "                                                                n_jobs = -1,\n",
    "                                                                scoring=\"neg_mean_squared_error\",\n",
    "                                                                input_cols=feature_cols,\n",
    "                                                                label_cols=target_cols,\n",
    "                                                                output_cols=output_cols\n",
    "                                                                )\n",
    "                            )\n",
    "                        ]      \n",
    "                    )\n",
    "\n",
    "                except Exception as e:\n",
    "                    return (f'Error with defining the pipeline: {e}')\n",
    "            '''\n",
    "            with tracer.start_as_current_span(\"train_test_split\"):\n",
    "                # Split the data into training and testing\n",
    "                train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "            '''\n",
    "\n",
    "            with tracer.start_as_current_span(\"fit_pipeline\"):\n",
    "                try:\n",
    "                    pipe.fit(train_df)\n",
    "                    telemetry.set_span_attribute(\"training.param_grid\", \"Fitting done\")\n",
    "                except Exception as e:\n",
    "                    return (f'Error with fitting pipeline: {e}')\n",
    "\n",
    "            # Model evaluation\n",
    "            with tracer.start_as_current_span(\"model_evaluation\"):\n",
    "                try:\n",
    "                    results = pipe.predict(test_df)\n",
    "                except Exception as e:\n",
    "                    return (f'Error with predicting with pipeline: {e}')\n",
    "\n",
    "                # Use Snowpark ML metrics to calculate MAPE and MSE\n",
    "                mape = mean_absolute_percentage_error(df=results, y_true_col_names=target_cols, y_pred_col_names=output_cols)\n",
    "                mse = mean_squared_error(df=results, y_true_col_names=target_cols, y_pred_col_names=output_cols)\n",
    "                telemetry.set_span_attribute(\"model.mape\", mape)\n",
    "                telemetry.set_span_attribute(\"model.mse\", mse)\n",
    "\n",
    "            # Model registration\n",
    "            with tracer.start_as_current_span(\"model_registration\"):\n",
    "                def set_model_version(registry_object,model_name, major_version=True):\n",
    "                    # See what we've logged so far, dynamically set the model version\n",
    "                    import numpy as np\n",
    "                    import json\n",
    "                \n",
    "                    model_list = registry_object.show_models()\n",
    "                    if len(model_list) == 0:\n",
    "                        return 'V1'\n",
    "                    \n",
    "                    model_list_filter = model_list[model_list['name'] ==  model_name]\n",
    "                    if len(model_list_filter) == 0:\n",
    "                        return 'V1'\n",
    "\n",
    "                    version_list_string = model_list_filter['versions'].iloc[0]\n",
    "                    version_list = json.loads(version_list_string)\n",
    "                    version_numbers = [float(s.replace('V', '')) for s in version_list]\n",
    "                    model_last_version = max(version_numbers)\n",
    "                    \n",
    "                    \n",
    "                    if np.isnan(model_last_version) == True:\n",
    "                        model_new_version = 'V1'\n",
    "\n",
    "                    elif np.isnan(model_last_version) == False and major_version == True:\n",
    "                        model_new_version = round(model_last_version + 1,2)\n",
    "                        model_new_version = 'V' + str(model_new_version)\n",
    "                        \n",
    "                    else:\n",
    "                        model_new_version = round(model_last_version + .1,2)\n",
    "                        model_new_version = 'V' + str(model_new_version)\n",
    "                        \n",
    "                    return model_new_version # This is the version we will use when we log the new model.\n",
    "\n",
    "                # Create model regisry object\n",
    "                try:\n",
    "                    model_registry = registry.Registry(session=session, database_name=session.get_current_database(), schema_name='ML_PIPE')\n",
    "\n",
    "                except Exception as e:\n",
    "                    return (f'Error with creating model registry object: {e}')\n",
    "                \n",
    "                try:\n",
    "                    target_cols = ['TOTAL_AMOUNT'] #edit\n",
    "                    feature_cols = [i for i in df.schema.names if i not in target_cols]\n",
    "                    X = train_df.select(feature_cols).limit(100)\n",
    "\n",
    "                    model_name = model_name\n",
    "                    version_name = set_model_version(model_registry, model_name, major_version=major_version)\n",
    "                    model_version = model_registry.log_model(\n",
    "                        model = pipe, \n",
    "                        model_name = model_name, \n",
    "                        version_name= f'\"{version_name}\"',\n",
    "                        sample_input_data=X,\n",
    "                        conda_dependencies=['snowflake-snowpark-python','snowflake-ml-python','scikit-learn', 'xgboost']\n",
    "                        )\n",
    "\n",
    "                    model_version.set_metric(metric_name='mean_abs_pct_err', value=mape)\n",
    "                    model_version.set_metric(metric_name='mean_sq_err', value=mse)\n",
    "                    telemetry.add_event(\"model_registered\", {\"version\": version_name})\n",
    "                \n",
    "                except Exception as e:\n",
    "                    return (f'Error with saving model to registry: {e}')\n",
    "                \n",
    "                try:\n",
    "                    session.sql(f'alter model {model_name} set default_version = \"{version_name}\";')\n",
    "                except Exception as e:\n",
    "                    return (f'Error with setting default version: {e}')\n",
    "\n",
    "            return f'Model {model_name} has been logged with version {version_name} and has a MAPE of {mape} and MSE of {mse}'\n",
    "\n",
    "        except Exception as e:\n",
    "            telemetry.add_event(\"pipeline_failure\", {\n",
    "                \"error\": str(e),\n",
    "                \"stack_trace\": traceback.format_exc()\n",
    "            })\n",
    "            raise  # Re-raise to preserve error handling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
